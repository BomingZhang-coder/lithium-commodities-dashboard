{"cells":[{"cell_type":"markdown","source":["# Time series - Li2Co3 zeroes calculations and statistics"],"metadata":{"id":"0lP9NsC71zHx"},"id":"0lP9NsC71zHx"},{"cell_type":"code","execution_count":null,"id":"3a96c2d4","metadata":{"id":"3a96c2d4"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n","import statsmodels.api as sm\n","from tabulate import tabulate"]},{"cell_type":"markdown","id":"xO2vfkkzHPM_","metadata":{"id":"xO2vfkkzHPM_"},"source":["# P1. Data Preprocess"]},{"cell_type":"code","execution_count":null,"id":"4c5a7272","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"4c5a7272","outputId":"ac47dcb2-c764-482b-9c60-80f9ec1f61e0","executionInfo":{"status":"error","timestamp":1719864209489,"user_tz":240,"elapsed":486,"user":{"displayName":"David Lu","userId":"17142883240638890203"}}},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/Lithium Carbonate 99%Min China Spot Historical Data (5).csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-0dc6a827abd0>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Data from 2017-05-10 to 2024-04-19\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mli2co3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'/content/Lithium Carbonate 99%Min China Spot Historical Data (5).csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mli2co3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mli2co3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# The date order need to be inverted (from early to late)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mli2co3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mli2co3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Lithium Carbonate 99%Min China Spot Historical Data (5).csv'"]}],"source":["# Data from 2017-05-10 to 2024-04-19\n","li2co3 = pd.read_csv(r'/content/Lithium Carbonate 99%Min China Spot Historical Data (5).csv')\n","li2co3['Date'] = pd.to_datetime(li2co3['Date'])\n","# The date order need to be inverted (from early to late)\n","li2co3 = li2co3.sort_values('Date')\n","li2co3.set_index('Date', inplace=True)\n","li2co3 = pd.DataFrame(li2co3[\"Price\"])\n","\n","\n","li2co3['Price'] = (li2co3['Price'].str.replace(\",\",\"\").astype(float))\n","na_count = li2co3['Price'].isna().sum()\n","print(\"Number of missing values:\", na_count)\n","if na_count > 0:\n","  li2co3 = li2co3.dropna(subset=['Price'])\n","\n","# daily log returns\n","li2co3['log_ret'] = np.log(li2co3['Price']).diff()\n","li2co3 = li2co3.dropna(subset=['log_ret'])"]},{"cell_type":"code","execution_count":null,"id":"e2f11f43","metadata":{"id":"e2f11f43"},"outputs":[],"source":["li2co3.head(10)"]},{"cell_type":"code","execution_count":null,"id":"c65ec064","metadata":{"id":"c65ec064"},"outputs":[],"source":["li2co3.tail(10)"]},{"cell_type":"markdown","id":"4G1AjjPSHTSG","metadata":{"id":"4G1AjjPSHTSG"},"source":["# P2. Weekly log return series and zero count series"]},{"cell_type":"code","execution_count":null,"id":"b51d1b9c","metadata":{"id":"b51d1b9c"},"outputs":[],"source":["Fridays = pd.DataFrame()\n","Thursdays = pd.DataFrame()\n","Wednesdays = pd.DataFrame()\n","Tuesdays = pd.DataFrame()\n","Mondays = pd.DataFrame()"]},{"cell_type":"code","execution_count":null,"id":"r9fRV5m6oUC_","metadata":{"id":"r9fRV5m6oUC_"},"outputs":[],"source":["def weekly_returns(data, chosen_day):\n","    # chosen_day = ['W-MON', 'W-TUE', 'W-WED', 'W-THU', 'W-FRI']\n","    weekly_log_return = data.groupby(pd.Grouper(freq=chosen_day))['log_ret'].sum()\n","    weekly_log_return = weekly_log_return.dropna()\n","    return weekly_log_return"]},{"cell_type":"code","execution_count":null,"id":"KgvGLL-fn0Vp","metadata":{"id":"KgvGLL-fn0Vp"},"outputs":[],"source":["def count_zero(df, chosen_day):\n","    # chosen_day = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']\n","    # Create a zero dummy series: 1 if 'log_ret' is zero, 0 otherwise\n","    data = df.copy()\n","    data['zero_dummy'] = (data['log_ret'] == 0).astype(int)\n","    # Compute the sum of past 22 days and past 5 days for zero_dummy\n","    data['zero_count_22'] = data['zero_dummy'].rolling(window=22).sum()\n","    data['zero_count_5'] = data['zero_dummy'].rolling(window=5).sum()\n","    data = data.dropna()\n","    # Extract chosen day\n","    data['day_of_week'] = data.index.day_name()\n","    chosendays_data = data[data['day_of_week'] == chosen_day]\n","\n","    # Select only the zero count columns and the index for Fridays\n","    chosendays_data = chosendays_data[['zero_count_22', 'zero_count_5']]\n","    chosendays_data['zero_count_22'] = chosendays_data['zero_count_22'].astype(int)\n","    chosendays_data['zero_count_5'] = chosendays_data['zero_count_5'].astype(int)\n","    return chosendays_data"]},{"cell_type":"code","execution_count":null,"id":"-M7JQw4OvcA2","metadata":{"id":"-M7JQw4OvcA2"},"outputs":[],"source":["# Friday to Friday\n","Fridays['Log_Return'] = weekly_returns(li2co3, 'W-FRI')\n","Fridays['Zero_Count_22'] = count_zero(li2co3, 'Friday')['zero_count_22']\n","Fridays['Zero_Count_5'] = count_zero(li2co3, 'Friday')['zero_count_5']\n","Fridays = Fridays.dropna()\n","\n","# Thursday to Thursday\n","Thursdays['Log_Return'] = weekly_returns(li2co3, 'W-THU')\n","Thursdays['Zero_Count_22'] = count_zero(li2co3, 'Thursday')['zero_count_22']\n","Thursdays['Zero_Count_5'] = count_zero(li2co3, 'Thursday')['zero_count_5']\n","Thursdays = Thursdays.dropna()\n","\n","# Wednesday to Wednesday\n","Wednesdays['Log_Return'] = weekly_returns(li2co3, 'W-WED')\n","Wednesdays['Zero_Count_22'] = count_zero(li2co3, 'Wednesday')['zero_count_22']\n","Wednesdays['Zero_Count_5'] = count_zero(li2co3, 'Wednesday')['zero_count_5']\n","Wednesdays = Wednesdays.dropna()\n","\n","# Tuesday to Tuesday\n","Tuesdays['Log_Return'] = weekly_returns(li2co3, 'W-TUE')\n","Tuesdays['Zero_Count_22'] = count_zero(li2co3, 'Tuesday')['zero_count_22']\n","Tuesdays['Zero_Count_5'] = count_zero(li2co3, 'Tuesday')['zero_count_5']\n","Tuesdays = Tuesdays.dropna()\n","\n","# Monday to Monday\n","Mondays['Log_Return'] = weekly_returns(li2co3, 'W-MON')\n","Mondays['Zero_Count_22'] = count_zero(li2co3, 'Monday')['zero_count_22']\n","Mondays['Zero_Count_5'] = count_zero(li2co3, 'Monday')['zero_count_5']\n","Mondays = Mondays.dropna()"]},{"cell_type":"code","execution_count":null,"id":"c5871a96","metadata":{"id":"c5871a96"},"outputs":[],"source":["print(\"Friday to Firday\")\n","print(Fridays.tail(10))\n","print(\"Thursday to Thursday\")\n","print(Thursdays.tail(10))\n","print(\"Wednesday to Wednesday\")\n","print(Wednesdays.tail(10))\n","print(\"Tuesday to Tuesday\")\n","print(Tuesdays.tail(10))\n","print(\"Monday to Monday\")\n","print(Mondays.tail(10))"]},{"cell_type":"markdown","id":"FVrSbD8cHf7j","metadata":{"id":"FVrSbD8cHf7j"},"source":["# P3. Summary statistics"]},{"cell_type":"code","execution_count":null,"id":"8o_j2Kzt_Xu6","metadata":{"id":"8o_j2Kzt_Xu6"},"outputs":[],"source":["summary_statistics_Fridays = Fridays.describe().loc[['mean', 'min', 'max', 'std']]\n","summary_statistics_Thursdays = Thursdays.describe().loc[['mean', 'min', 'max', 'std']]\n","summary_statistics_Wednesdays = Wednesdays.describe().loc[['mean', 'min', 'max', 'std']]\n","summary_statistics_Tuesdays = Tuesdays.describe().loc[['mean', 'min', 'max', 'std']]\n","summary_statistics_Mondays = Mondays.describe().loc[['mean', 'min', 'max', 'std']]\n","\n","print(\"Friday to Firday\")\n","print(summary_statistics_Fridays)\n","print(\"Thursday to Thursday\")\n","print(summary_statistics_Thursdays)\n","print(\"Wednesday to Wednesday\")\n","print(summary_statistics_Wednesdays)\n","print(\"Tuesday to Tuesday\")\n","print(summary_statistics_Tuesdays)\n","print(\"Monday to Monday\")\n","print(summary_statistics_Mondays)"]},{"cell_type":"markdown","id":"Mb1Xq4QIHbuu","metadata":{"id":"Mb1Xq4QIHbuu"},"source":["# P4. Autocorrelogram and partial autocorrelogram for daily return"]},{"cell_type":"code","execution_count":null,"id":"6448d84b","metadata":{"id":"6448d84b"},"outputs":[],"source":["# Daily return\n","fig, axs = plt.subplots(1, 1, figsize=(10, 5))\n","axs.bar(li2co3.index, li2co3['log_ret'], width=2.5)\n","plt.axhline(0, linewidth=0.8, color='k')\n","plt.xlabel('Date')\n","plt.ylabel('Log Returns')\n","plt.title('Li2CO3 99% Daily Log Return')\n","plt.show()\n","\n","# Plot autocorrelation and partial autocorrelation\n","acf = plot_acf(li2co3['log_ret'], lags=20, alpha=0.1, title='Li2CO3 99% Autocorrelation for Daily Return')\n","pacf = plot_pacf(li2co3['log_ret'], lags=20, alpha=0.1, method='ywm', title='Li2CO3 99% Partial Autocorrelation for Daily Return')"]},{"cell_type":"markdown","id":"j2P6qwS2O3AW","metadata":{"id":"j2P6qwS2O3AW"},"source":["# P5. Autocorrelogram and partial autocorrelogram for weekly return (use Fridays as example)"]},{"cell_type":"code","execution_count":null,"id":"2p3tBjvlLzQ2","metadata":{"id":"2p3tBjvlLzQ2"},"outputs":[],"source":["# Weekly return, use Friday to Friday as example\n","fig, axs = plt.subplots(1, 1, figsize=(10, 5))\n","axs.bar(Fridays.index, Fridays['Log_Return'], width=2.5)\n","plt.axhline(0, linewidth=0.8, color='k')\n","plt.xlabel('Date')\n","plt.ylabel('Log Returns')\n","plt.title('Li2CO3 99% Weekly Log Return')\n","plt.show()\n","\n","# Plot autocorrelation and partial autocorrelation\n","acf = plot_acf(Fridays['Log_Return'], lags=20, alpha=0.1, title='Li2CO3 99% Autocorrelation for Weekly Return')\n","pacf = plot_pacf(Fridays['Log_Return'], lags=20, alpha=0.1, method='ywm', title='Li2CO3 99% Partial Autocorrelation for Weekly Return')"]},{"cell_type":"markdown","id":"Jmgccze5H77i","metadata":{"id":"Jmgccze5H77i"},"source":["# P6. Two AR(2) models for daily return"]},{"cell_type":"code","execution_count":null,"id":"46ae976d","metadata":{"id":"46ae976d"},"outputs":[],"source":["def estimate_ar2_model(df, lags):\n","    \"\"\"\n","    Estimate a basic AR(2) model for returns.\n","\n","    Parameters:\n","    - data: DataFrame containing the log returns series under 'log_ret'.\n","    - lags: Number of lags to use for HAC standard errors.\n","\n","    Returns:\n","    - model: OLS regression results containing the fitted model.\n","    \"\"\"\n","\n","    data = df.copy()\n","    data['log_ret_lag1'] = data['log_ret'].shift(1)\n","    data['log_ret_lag2'] = data['log_ret'].shift(2)\n","\n","    data.dropna(inplace=True)\n","\n","    # Define the variables and add a constant term for the intercept (alpha)\n","    X = data[['log_ret_lag1', 'log_ret_lag2']]\n","    X = sm.add_constant(X)  # Adds a constant column to input data set\n","    Y = data['log_ret']\n","\n","    # Fit the AR(2) model\n","    model = sm.OLS(Y, X).fit(cov_type='HAC', cov_kwds={'maxlags': lags})\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"id":"rJkhDQJcDtX2","metadata":{"id":"rJkhDQJcDtX2"},"outputs":[],"source":["# lags=5 acording to the daily pacf\n","daily_return_ar2_model = estimate_ar2_model(li2co3, lags=5)"]},{"cell_type":"code","execution_count":null,"id":"lrhUgCB9G-Ry","metadata":{"id":"lrhUgCB9G-Ry"},"outputs":[],"source":["print(daily_return_ar2_model.summary())"]},{"cell_type":"code","execution_count":null,"id":"BwSY5qT_Ebgl","metadata":{"id":"BwSY5qT_Ebgl"},"outputs":[],"source":["def estimate_ar2_model_with_zero_dummy(df, lags):\n","    \"\"\"\n","    Estimate an AR(2) model for log returns with modifications to account for zero dummies.\n","\n","    Parameters:\n","    - data: DataFrame containing the log returns 'log_ret'.\n","    - lags: Number of lags to use for HAC standard errors.\n","\n","    Returns:\n","    - model: OLS regression results containing the fitted model.\n","    \"\"\"\n","\n","    data = df.copy()\n","    data['zero_dummy'] = (data['log_ret'] == 0).astype(int)\n","\n","    # Generate lagged return series\n","    data['log_ret_lag1'] = data['log_ret'].shift(1)\n","    data['log_ret_lag2'] = data['log_ret'].shift(2)\n","\n","    # Generate interaction terms\n","    data['log_ret_lag1_zero'] = data['log_ret_lag1'] * data['zero_dummy']\n","    data['log_ret_lag2_zero'] = data['log_ret_lag2'] * data['zero_dummy']\n","\n","    data.dropna(inplace=True)\n","\n","    # Define the new model with additional interaction terms\n","    X = data[['zero_dummy', 'log_ret_lag1', 'log_ret_lag2', 'log_ret_lag1_zero', 'log_ret_lag2_zero']]\n","    X = sm.add_constant(X)\n","    Y = data['log_ret']\n","\n","    # Fit the AR(2) model\n","    model = sm.OLS(Y, X).fit(cov_type='HAC', cov_kwds={'maxlags': lags})\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"id":"W_kXkC45G0sE","metadata":{"id":"W_kXkC45G0sE"},"outputs":[],"source":["# lags=5 acording to the daily pacf\n","daily_return_ar2_model_with_zero_dummy = estimate_ar2_model_with_zero_dummy(li2co3, lags = 5)"]},{"cell_type":"code","execution_count":null,"id":"9nYyNm09HGJo","metadata":{"id":"9nYyNm09HGJo"},"outputs":[],"source":["print(daily_return_ar2_model_with_zero_dummy.summary())"]},{"cell_type":"markdown","id":"P0qVjKPOOprG","metadata":{"id":"P0qVjKPOOprG"},"source":["# **New** P7. Four AR(2) models for weekly return (use Fridays as example)"]},{"cell_type":"markdown","source":["## Model1: Standard AR(2) model"],"metadata":{"id":"vHhGQYj0DQhI"},"id":"vHhGQYj0DQhI"},{"cell_type":"code","execution_count":null,"id":"19HoZ9sgPIKF","metadata":{"id":"19HoZ9sgPIKF"},"outputs":[],"source":["def estimate_ar2_model(df, lags):\n","    \"\"\"\n","    Estimate a basic AR(2) model for returns.\n","\n","    Parameters:\n","    - data: DataFrame containing the log returns series under 'Log_Return'.\n","    - lags: Number of lags to use for HAC standard errors.\n","\n","    Returns:\n","    - model: OLS regression results containing the fitted model.\n","    \"\"\"\n","\n","    data = df.copy()\n","    data['log_ret_lag1'] = data['Log_Return'].shift(1)\n","    data['log_ret_lag2'] = data['Log_Return'].shift(2)\n","\n","    data.dropna(inplace=True)\n","\n","    # Define the variables and add a constant term for the intercept (alpha)\n","    X = data[['log_ret_lag1', 'log_ret_lag2']]\n","    X = sm.add_constant(X)  # Adds a constant column to input data set\n","    Y = data['Log_Return']\n","\n","    # Fit the AR(2) model\n","    model = sm.OLS(Y, X).fit(cov_type='HAC', cov_kwds={'maxlags': lags})\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"id":"1zTGyZhJICPo","metadata":{"id":"1zTGyZhJICPo"},"outputs":[],"source":["# lags=4 acording to the weekly pacf\n","weekly_return_ar2_model = estimate_ar2_model(Fridays, lags=4)\n","\n","print(weekly_return_ar2_model.summary())"]},{"cell_type":"markdown","source":["## Model2: AR(2) model using interaction with the weekly zero count series"],"metadata":{"id":"-rq9o2e_DdfT"},"id":"-rq9o2e_DdfT"},{"cell_type":"code","execution_count":null,"id":"67Echgm-QfxO","metadata":{"id":"67Echgm-QfxO"},"outputs":[],"source":["def estimate_ar2_model_with_weekly_zero(df, lags):\n","    \"\"\"\n","    Estimate an AR(2) model for weekly log returns, incorporating\n","    weekly zero count series as interaction effect,\n","    and allowing for specification of lags for HAC standard errors.\n","\n","    Parameters:\n","    - data: DataFrame containing the weekly log returns under 'Log_Return',\n","         and the weekly zero count series 'Zero_Count_5'.\n","    - lags: Maximum number of lags to use for HAC standard errors.\n","\n","    Returns:\n","    - model: OLS regression results containing the fitted model with HAC standard errors.\n","    \"\"\"\n","\n","    data = df.copy()\n","    # Generate lagged return series\n","    data['Log_Return_Lag1'] = data['Log_Return'].shift(1)\n","    data['Log_Return_Lag2'] = data['Log_Return'].shift(2)\n","\n","    # Generate interaction terms for lagged returns and 'Zero_Count_5'\n","    data['Log_Return_Lag1_Zero5'] = data['Log_Return_Lag1'] * data['Zero_Count_5']\n","    data['Log_Return_Lag2_Zero5'] = data['Log_Return_Lag2'] * data['Zero_Count_5']\n","\n","    # Drop any rows with NaN values that were created by lagging\n","    data.dropna(inplace=True)\n","\n","    # Define the model with additional interaction terms\n","    X = data[['Zero_Count_5', 'Log_Return_Lag1', 'Log_Return_Lag2',\n","              'Log_Return_Lag1_Zero5', 'Log_Return_Lag2_Zero5']]\n","    X = sm.add_constant(X)\n","    Y = data['Log_Return']\n","\n","    # Fit the model with HAC standard errors\n","    model = sm.OLS(Y, X).fit(cov_type='HAC', cov_kwds={'maxlags': lags})\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"id":"t07umcPxRIHK","metadata":{"id":"t07umcPxRIHK"},"outputs":[],"source":["# lags=4 acording to the weekly pacf\n","ar2_model_with_weekly_zero = estimate_ar2_model_with_weekly_zero(Fridays, lags=4)\n","\n","print(ar2_model_with_weekly_zero.summary())"]},{"cell_type":"markdown","source":["## Model3: AR(2) model using interaction with the monthly zero count series"],"metadata":{"id":"Isoimbs7Dq4A"},"id":"Isoimbs7Dq4A"},{"cell_type":"code","source":["def estimate_ar2_model_with_monthly_zero(df, lags):\n","    \"\"\"\n","    Estimate an AR(2) model for weekly log returns, incorporating\n","    monthly zero count series as interaction effect,\n","    and allowing for specification of lags for HAC standard errors.\n","\n","    Parameters:\n","    - data: DataFrame containing the weekly log returns under 'Log_Return',\n","         and the monthly zero count series 'Zero_Count_22'.\n","    - lags: Maximum number of lags to use for HAC standard errors.\n","\n","    Returns:\n","    - model: OLS regression results containing the fitted model with HAC standard errors.\n","    \"\"\"\n","\n","    data = df.copy()\n","    # Generate lagged return series\n","    data['Log_Return_Lag1'] = data['Log_Return'].shift(1)\n","    data['Log_Return_Lag2'] = data['Log_Return'].shift(2)\n","\n","    # Generate interaction terms for lagged returns and 'Zero_Count_22'\n","    data['Log_Return_Lag1_Zero22'] = data['Log_Return_Lag1'] * data['Zero_Count_22']\n","    data['Log_Return_Lag2_Zero22'] = data['Log_Return_Lag2'] * data['Zero_Count_22']\n","\n","    # Drop any rows with NaN values that were created by lagging\n","    data.dropna(inplace=True)\n","\n","    # Define the model with additional interaction terms\n","    X = data[['Zero_Count_22', 'Log_Return_Lag1', 'Log_Return_Lag2',\n","              'Log_Return_Lag1_Zero22', 'Log_Return_Lag2_Zero22']]\n","    X = sm.add_constant(X)\n","    Y = data['Log_Return']\n","\n","    # Fit the model with HAC standard errors\n","    model = sm.OLS(Y, X).fit(cov_type='HAC', cov_kwds={'maxlags': lags})\n","\n","    return model"],"metadata":{"id":"8GL5FBPGMBEd"},"id":"8GL5FBPGMBEd","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# lags=4 acording to the weekly pacf\n","ar2_model_with_monthly_zero = estimate_ar2_model_with_monthly_zero(Fridays, lags=4)\n","\n","print(ar2_model_with_monthly_zero.summary())"],"metadata":{"id":"IP3MNIlyMoDu"},"id":"IP3MNIlyMoDu","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" ## Model4: AR(2) model **separately** using interactions with the short-term illiquidity and long-term illiquidity variable"],"metadata":{"id":"6smDxfKsD9dq"},"id":"6smDxfKsD9dq"},{"cell_type":"code","source":["def estimate_ar2_model_separate_illiquidity(df, lags):\n","    \"\"\"\n","    Estimate a complex AR(2) model for log returns, incorporating\n","    separate interactions with two types of zero counts.\n","    The formula incorporates separate interactions for different lags\n","    with different term-length zero count measures.\n","\n","    Parameters:\n","    - data: DataFrame containing the log returns under 'Log_Return',\n","         two zero count series 'Zero_Count_5' and 'Zero_Count_22'.\n","    - maxlags: Maximum number of lags to use for HAC standard errors.\n","\n","    Returns:\n","    - model: OLS regression results containing the fitted model with HAC standard errors.\n","    \"\"\"\n","\n","    data = df.copy()\n","    # Generate lagged return series\n","    data['Log_Return_Lag1'] = data['Log_Return'].shift(1)\n","    data['Log_Return_Lag2'] = data['Log_Return'].shift(2)\n","\n","    # Generate separate interaction terms for lagged returns and zero counts\n","    data['Log_Return_Lag1_Zero5'] = data['Log_Return_Lag1'] * data['Zero_Count_5']\n","    data['Log_Return_Lag2_Zero22'] = data['Log_Return_Lag2'] * data['Zero_Count_22']\n","\n","    # Drop any rows with NaN values that were created by lagging\n","    data.dropna(inplace=True)\n","\n","    # Define the model with additional interaction terms\n","    X = data[['Zero_Count_5', 'Zero_Count_22', 'Log_Return_Lag1', 'Log_Return_Lag2',\n","              'Log_Return_Lag1_Zero5', 'Log_Return_Lag2_Zero22']]\n","    X = sm.add_constant(X)\n","    Y = data['Log_Return']\n","\n","    # Fit the model with HAC standard errors\n","    model = sm.OLS(Y, X).fit(cov_type='HAC', cov_kwds={'maxlags': lags})\n","\n","    return model"],"metadata":{"id":"PPXEZ0nBOPer"},"id":"PPXEZ0nBOPer","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# lags=4 acording to the weekly pacf\n","ar2_model_with_separate_illiquidity = estimate_ar2_model_separate_illiquidity(Fridays, lags=4)\n","\n","print(ar2_model_with_separate_illiquidity.summary())"],"metadata":{"id":"RUXaZJMtSL5C"},"id":"RUXaZJMtSL5C","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Comparsion among last three models"],"metadata":{"id":"HW-2MiGpEiyV"},"id":"HW-2MiGpEiyV"},{"cell_type":"code","source":["model1_results = ar2_model_with_weekly_zero\n","model2_results = ar2_model_with_monthly_zero\n","model3_results = ar2_model_with_separate_illiquidity\n","\n","# Create a DataFrame to summarize the fit statistics\n","summary_stats = pd.DataFrame({\n","    'Model': ['Model with Weekly Zero', 'Model with Monthly Zero', 'Model with Separate Illiquidity'],\n","    'R-squared': [model1_results.rsquared, model2_results.rsquared, model3_results.rsquared],\n","    'Adj. R-squared': [model1_results.rsquared_adj, model2_results.rsquared_adj, model3_results.rsquared_adj],\n","    'AIC': [model1_results.aic, model2_results.aic, model3_results.aic],\n","    'BIC': [model1_results.bic, model2_results.bic, model3_results.bic],\n","    'F-statistic': [model1_results.fvalue, model2_results.fvalue, model3_results.fvalue]\n","})\n","\n","# Print the summary statistics using tabulate\n","print(tabulate(summary_stats, headers='keys', tablefmt='pretty', showindex=False))"],"metadata":{"id":"Ag0mfb5iUnFc"},"id":"Ag0mfb5iUnFc","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Interpretation\n","\n","**R-squared** & **Adj. R-squared**: The proportion of the variance in the dependent variable that is predictable from the independent variables. The higher these values, the better the model explains the variability of the response variable. From the table, the *Model with Separate Illiquidity* has both the highest R-squared and the highest Adjusted R-squared, which means it is the best model that explains the highest proportion of variance in the weekly log return series among the last three models.\n","\n","\n","**AIC** & **BIC**: Both criteria help in model selection where lower values generally indicate a better model. AIC shows the goodness of fit with a penalty for the number of parameters to avoid overfitting, and BIC considers a larger penalty for models with more parameters. The *Model with Separate Illiquidity* has both the lowest AIC and the lowest BIC, which means it is the best model from a complexity-fit trade-off perspective.\n","\n","\n","**F-statistic**: Indicates the overall significance of the regression AR(2) model. The higher the F-statistic, the more significant the model is. The *Model with Separate Illiquidity* has the highest F-statistic, which means it is statistically the most significant model in terms of the contribution of the explanatory variables used in the model.\n","\n","\n","## Conclusion\n","*Model with Separate Illiquidity* has advantages on all three aspects:\n","explaining the variance in the weekly log return; balance between model complexity and fit; and also, the prediction ability, because the statistical significance of this model's explanatory variables is highest, making it potentially more reliable when making predictions or inferences.\n","\n","Besides, *Model with Weekly Zero* is the second best model, where its\n","variance explaining ability and complexity-fit balance are all very close\n","to the *Model with Separate Illiquidity*.\n","\n","\n","The conclusion is just for Friday to Friday return series, it might\n","be different for other days."],"metadata":{"id":"fa0Yq7uBZGqc"},"id":"fa0Yq7uBZGqc"}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":5}