{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from word2number import w2n\n",
    "\n",
    "from datetime import datetime, timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./lithium_dataset\"\n",
    "cleaned_data_dir = \"./lithium_dataset/cleaned_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Price Data Ingestion + Processing\n",
    "- Consider lithium future, lithium metal spot, Li2CO3, and LIOH prices are potential regressands\n",
    "- Clean each by calculating all available log returns with lags 1, 7, and 14 days\n",
    "- According to Professor Geard, we'll only use data from 2021, which would include on average 500-600 datapoints\n",
    "- Store them in jerry/lithium_dataset/cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string_to_numeric(s):\n",
    "    \"\"\"\n",
    "    Convert strings like '5.74M' or '532.50K' to numeric values\n",
    "    \"\"\"\n",
    "    s = str(s).replace(',', '')  # Remove commas\n",
    "    if 'M' in s:\n",
    "        return float(s.replace('M', '')) * 1e6\n",
    "    elif 'K' in s:\n",
    "        return float(s.replace('K', '')) * 1e3\n",
    "    else:\n",
    "        return float(s)\n",
    "\n",
    "\n",
    "def process_price_dataframe(df, price_cols, date_format, lags):\n",
    "\n",
    "    # Convert columns in price_cols from string to numeric\n",
    "    for col in price_cols:\n",
    "        if type(df.loc[0,col]) == str:\n",
    "            df[col] = df[col].apply(convert_string_to_numeric)\n",
    "        else: \n",
    "            df[col] = df[col].astype(float)\n",
    "\n",
    "    # Processing Dates\n",
    "    df['Date'] = df['Date'].str.replace('.', '')\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format=date_format)\n",
    "    for lag in lags:\n",
    "        df[f'lag_{lag}'] = df.loc[:,'Date'] - timedelta(days=lag)\n",
    "    df.set_index('Date', inplace=True)\n",
    "    df = df.sort_index(ascending=True)\n",
    "    for lag in lags:\n",
    "        df[f'lag_{lag}_price'] = df[f'lag_{lag}'].apply(lambda x: df.loc[x,'Close'] if x in df.index else np.NaN)\n",
    "        df = df.drop(columns=[f'lag_{lag}'])\n",
    "        df[f'lag_{lag}_log_return'] = np.log(df['Close'] / df[f'lag_{lag}_price'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/site-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/site-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/site-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/site-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/site-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>lag_1_price</th>\n",
       "      <th>lag_1_log_return</th>\n",
       "      <th>lag_7_price</th>\n",
       "      <th>lag_7_log_return</th>\n",
       "      <th>lag_14_price</th>\n",
       "      <th>lag_14_log_return</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005-01-04</th>\n",
       "      <td>23200.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-05</th>\n",
       "      <td>23200.0</td>\n",
       "      <td>23200.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-06</th>\n",
       "      <td>23200.0</td>\n",
       "      <td>23200.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-07</th>\n",
       "      <td>23200.0</td>\n",
       "      <td>23200.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-10</th>\n",
       "      <td>23200.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-16</th>\n",
       "      <td>265000.0</td>\n",
       "      <td>258000.0</td>\n",
       "      <td>0.026770</td>\n",
       "      <td>195500.0</td>\n",
       "      <td>0.304169</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-17</th>\n",
       "      <td>274000.0</td>\n",
       "      <td>265000.0</td>\n",
       "      <td>0.033398</td>\n",
       "      <td>208500.0</td>\n",
       "      <td>0.273189</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-18</th>\n",
       "      <td>290000.0</td>\n",
       "      <td>274000.0</td>\n",
       "      <td>0.056753</td>\n",
       "      <td>225000.0</td>\n",
       "      <td>0.253781</td>\n",
       "      <td>179500.0</td>\n",
       "      <td>0.479706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-19</th>\n",
       "      <td>292000.0</td>\n",
       "      <td>290000.0</td>\n",
       "      <td>0.006873</td>\n",
       "      <td>242000.0</td>\n",
       "      <td>0.187816</td>\n",
       "      <td>180500.0</td>\n",
       "      <td>0.481023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>258000.0</td>\n",
       "      <td>-inf</td>\n",
       "      <td>190000.0</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3795 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Close  lag_1_price  lag_1_log_return  lag_7_price  \\\n",
       "Date                                                               \n",
       "2005-01-04   23200.0          NaN               NaN          NaN   \n",
       "2005-01-05   23200.0      23200.0          0.000000          NaN   \n",
       "2005-01-06   23200.0      23200.0          0.000000          NaN   \n",
       "2005-01-07   23200.0      23200.0          0.000000          NaN   \n",
       "2005-01-10   23200.0          NaN               NaN          NaN   \n",
       "...              ...          ...               ...          ...   \n",
       "2023-05-16  265000.0     258000.0          0.026770     195500.0   \n",
       "2023-05-17  274000.0     265000.0          0.033398     208500.0   \n",
       "2023-05-18  290000.0     274000.0          0.056753     225000.0   \n",
       "2023-05-19  292000.0     290000.0          0.006873     242000.0   \n",
       "2023-05-22       0.0          NaN               NaN     258000.0   \n",
       "\n",
       "            lag_7_log_return  lag_14_price  lag_14_log_return  \n",
       "Date                                                           \n",
       "2005-01-04               NaN           NaN                NaN  \n",
       "2005-01-05               NaN           NaN                NaN  \n",
       "2005-01-06               NaN           NaN                NaN  \n",
       "2005-01-07               NaN           NaN                NaN  \n",
       "2005-01-10               NaN           NaN                NaN  \n",
       "...                      ...           ...                ...  \n",
       "2023-05-16          0.304169           NaN                NaN  \n",
       "2023-05-17          0.273189           NaN                NaN  \n",
       "2023-05-18          0.253781      179500.0           0.479706  \n",
       "2023-05-19          0.187816      180500.0           0.481023  \n",
       "2023-05-22              -inf      190000.0               -inf  \n",
       "\n",
       "[3795 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressands = ['60C-LTCX', '99C-LTCB', '995C-LTCB', '99MIN-LTMT', '999MIN-LTMT']\n",
    "lags = [1,7,14]\n",
    "all_data = {k: None for k in regressands}\n",
    "\n",
    "all_data['li_future'] = pd.read_excel(f\"{data_dir}/Historical Prices (9).xlsx\")[['Date', 'Close']]\n",
    "all_data['li2co3'] = pd.read_csv(f\"{data_dir}/Lithium Carbonate (wind database).csv\").rename(columns={'Li2CO3 99%': 'Close'})\n",
    "all_data['lioh'] = pd.read_csv(f\"{data_dir}/Lithium Hydroxide (wind database).csv\").rename(columns={'LiOH 56.5%': 'Close'})\n",
    "for reg in regressands:\n",
    "    all_data[reg] = pd.read_csv(f\"{data_dir}/{reg}.csv\")[['Date', 'Price']].rename(columns={'Price':'Close'})\n",
    "\n",
    "for reg in all_data.keys():\n",
    "    if reg == 'li_future':\n",
    "        all_data[reg] = process_price_dataframe(all_data[reg], price_cols=['Close'], date_format='%b %d, %Y', lags=lags)\n",
    "    elif reg in ['li2co3', 'lioh']:\n",
    "        all_data[reg] = process_price_dataframe(all_data[reg], price_cols=['Close'], date_format='%Y-%m-%d', lags=lags)\n",
    "    else:\n",
    "        all_data[reg] = process_price_dataframe(all_data[reg], price_cols=['Close'], date_format='%m/%d/%Y', lags=lags)\n",
    "    all_data[reg].to_csv(f\"{cleaned_data_dir}/{reg}_cleaned.csv\")\n",
    "\n",
    "all_data['li2co3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News Data Ingestion\n",
    "- Use lithium_merged.csv as the source of all Lithium-related news\n",
    "- Only keep news url and tone, which would be used to fetch all text contents --> NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>tone</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-05-02 13:30:00</th>\n",
       "      <td>https://www.insiderfinancial.com/lithium-x-ene...</td>\n",
       "      <td>0.818554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-02 15:30:00</th>\n",
       "      <td>http://www.prnewswire.com/news-releases/hotter...</td>\n",
       "      <td>-1.374570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-03 06:00:00</th>\n",
       "      <td>http://www.einnews.com/pr_news/379071017/power...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-03 11:00:00</th>\n",
       "      <td>http://www.einnews.com/pr_news/379118179/nemas...</td>\n",
       "      <td>-0.092593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-03 13:30:00</th>\n",
       "      <td>http://www.finanznachrichten.de/nachrichten-20...</td>\n",
       "      <td>0.207469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-30 22:00:00</th>\n",
       "      <td>https://www.sandiegoreader.com/news/2023/may/3...</td>\n",
       "      <td>-4.336043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-31 00:30:00</th>\n",
       "      <td>https://www.havasunews.com/nation/could-the-ru...</td>\n",
       "      <td>-3.505911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-31 12:00:00</th>\n",
       "      <td>https://www.finanznachrichten.de/nachrichten-2...</td>\n",
       "      <td>-0.213447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-31 13:00:00</th>\n",
       "      <td>https://www.finanznachrichten.de/nachrichten-2...</td>\n",
       "      <td>3.892028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-31 13:15:00</th>\n",
       "      <td>https://www.phoenixherald.com/news/273849187/a...</td>\n",
       "      <td>3.900256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6686 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                   url  \\\n",
       "DATE                                                                     \n",
       "2017-05-02 13:30:00  https://www.insiderfinancial.com/lithium-x-ene...   \n",
       "2017-05-02 15:30:00  http://www.prnewswire.com/news-releases/hotter...   \n",
       "2017-05-03 06:00:00  http://www.einnews.com/pr_news/379071017/power...   \n",
       "2017-05-03 11:00:00  http://www.einnews.com/pr_news/379118179/nemas...   \n",
       "2017-05-03 13:30:00  http://www.finanznachrichten.de/nachrichten-20...   \n",
       "...                                                                ...   \n",
       "2023-05-30 22:00:00  https://www.sandiegoreader.com/news/2023/may/3...   \n",
       "2023-05-31 00:30:00  https://www.havasunews.com/nation/could-the-ru...   \n",
       "2023-05-31 12:00:00  https://www.finanznachrichten.de/nachrichten-2...   \n",
       "2023-05-31 13:00:00  https://www.finanznachrichten.de/nachrichten-2...   \n",
       "2023-05-31 13:15:00  https://www.phoenixherald.com/news/273849187/a...   \n",
       "\n",
       "                         tone  \n",
       "DATE                           \n",
       "2017-05-02 13:30:00  0.818554  \n",
       "2017-05-02 15:30:00 -1.374570  \n",
       "2017-05-03 06:00:00  0.000000  \n",
       "2017-05-03 11:00:00 -0.092593  \n",
       "2017-05-03 13:30:00  0.207469  \n",
       "...                       ...  \n",
       "2023-05-30 22:00:00 -4.336043  \n",
       "2023-05-31 00:30:00 -3.505911  \n",
       "2023-05-31 12:00:00 -0.213447  \n",
       "2023-05-31 13:00:00  3.892028  \n",
       "2023-05-31 13:15:00  3.900256  \n",
       "\n",
       "[6686 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "news = pd.read_csv(f'{data_dir}/lithium_merged.csv')\n",
    "news.set_index('DATE', inplace=True)\n",
    "news.index = pd.to_datetime(news.index, format = '%Y%m%d%H%M%S')\n",
    "news = news.sort_index(ascending = True)['2017-05-02':]\n",
    "news = news[['DocumentIdentifier', 'V2Tone']].rename(columns={'DocumentIdentifier': 'url', 'V2Tone': 'tone'})\n",
    "\n",
    "display(news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine News & Price Data to Generate Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Combine news and corresponding price data\n",
    "news['date'] = [datetime.date(d).strftime('%Y-%m-%d') for d in news.index] \n",
    "def get_log_return(price_df, lag, timestamp):\n",
    "    if timestamp in price_df.index:\n",
    "        return price_df.loc[timestamp, lag]\n",
    "    else:\n",
    "        return np.NaN\n",
    "\n",
    "for col in ['Close', 'lag_1_log_return', 'lag_7_log_return', 'lag_14_log_return']:\n",
    "    news[col] = news['date'].apply(lambda x: get_log_return(all_data['li_future'], col, x))\n",
    "\n",
    "\n",
    "### Generate labels using the given column & thresholds\n",
    "thresholds = [0,0]\n",
    "def num_to_label(num):\n",
    "    if num < thresholds[0]:\n",
    "        return 'SELL'\n",
    "    elif num > thresholds[1]:\n",
    "        return 'BUY'\n",
    "    else:\n",
    "        return 'HOLD'\n",
    "\n",
    "for lag in [1,7,14]:\n",
    "    news[f'lag_{lag}_label'] = news[f'lag_{lag}_log_return'].apply(lambda x: num_to_label(x))\n",
    "\n",
    "\n",
    "### Final cleaning & saving data\n",
    "news = news.drop(columns=['date'])['2021-01-01':]\n",
    "news = news.dropna()\n",
    "news.to_csv(f\"{cleaned_data_dir}/news_price_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News Text Preprocessing: \n",
    "- get news content using newspaper library\n",
    "- clean content using Spacy, NLTK, etc. Steps include:\n",
    "    - Remove noise including urls, accented characters, mentions & hashtags, punctuations, and extra whitespaces\n",
    "    - Convert all content to lowercase\n",
    "    - Remove stopwords defined by spacy & manual input\n",
    "    - Remove numbers\n",
    "    - Lemmatize the text\n",
    "- Store all text content corresponding to each news article in a large df, removing unfetchable news --> save csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_nltk = nltk.corpus.stopwords.words(\"english\")\n",
    "sw_spacy = spacy_nlp.Defaults.stop_words\n",
    "exclude_stopwords = ['one', 'two', 'three', 'four', 'five', 'six', 'eight', 'nine', 'ten', 'twelve', 'fifteen', 'twenty', 'forty', 'fifty', 'sixty', 'hundred', 'not', ]\n",
    "include_stopwords = ['email', 'phone', 'contact', 'information', 'link', 'tel']\n",
    "sw_spacy = [word for word in sw_spacy if word not in exclude_stopwords]\n",
    "sw_spacy = sw_spacy + include_stopwords\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "news_text = news.copy()\n",
    "news_text['text'] = news_text.loc[:,'url'].apply(lambda x: url_to_clean_text(url=x, stopwords=sw_spacy, lemmatizer=wordnet_lemmatizer, remove_num_or_not=True, language=None))\n",
    "news_text = news_text[(news_text['text'] != 1) & (news_text['text'] != '')]\n",
    "\n",
    "display(news_text)\n",
    "display(news_text.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_text.to_csv('news_text.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
